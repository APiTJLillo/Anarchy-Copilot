name: Performance Tests

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      baseline_update:
        description: 'Update performance baseline'
        required: false
        default: 'false'

jobs:
  performance-tests:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0  # Fetch all history for baseline comparisons
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Cache performance baselines
      uses: actions/cache@v3
      with:
        path: .performance_baselines
        key: perf-baseline-${{ runner.os }}-${{ hashFiles('tests/proxy/server/handlers/test_middleware_regression.py') }}
        restore-keys: |
          perf-baseline-${{ runner.os }}-
    
    - name: Prepare environment
      run: |
        mkdir -p .performance_baselines
        mkdir -p test-results/performance
    
    - name: Run performance tests
      run: |
        python -m pytest \
          tests/proxy/server/handlers/test_middleware_performance.py \
          tests/proxy/server/handlers/test_middleware_regression.py \
          --benchmark-only \
          --html=test-results/performance/report.html \
          --self-contained-html \
          -v
      env:
        PERFORMANCE_BASELINE_DIR: .performance_baselines
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        CI: true
    
    - name: Check for regressions
      run: |
        python -c '
        import json
        import sys
        from pathlib import Path

        def check_regression():
            results = Path("test-results/performance/results.json")
            if not results.exists():
                print("No performance results found")
                return 1
                
            with open(results) as f:
                data = json.load(f)
                
            regressions = data.get("regressions", {})
            if regressions:
                print("Performance regressions detected:")
                for metric, details in regressions.items():
                    print(f"{metric}:")
                    print(f"  Baseline: {details['baseline']:.2f}")
                    print(f"  Current:  {details['current']:.2f}")
                    print(f"  Degradation: {details['degradation']:.1f}%")
                return 1
            return 0
            
        sys.exit(check_regression())
        '
    
    - name: Update baseline
      if: |
        github.event_name == 'workflow_dispatch' && 
        github.event.inputs.baseline_update == 'true'
      run: |
        python -c '
        from pathlib import Path
        import shutil
        import datetime

        baseline_dir = Path(".performance_baselines")
        results_file = Path("test-results/performance/results.json")
        
        if results_file.exists():
            new_baseline = baseline_dir / f"baseline_{datetime.datetime.now():%Y%m%d_%H%M%S}.json"
            shutil.copy(results_file, new_baseline)
            print(f"Updated baseline: {new_baseline}")
        '
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: performance-test-results
        path: |
          test-results/performance/
          .performance_baselines/
    
    - name: Comment PR with Results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const results = JSON.parse(fs.readFileSync('test-results/performance/results.json', 'utf8'));
          
          let comment = '## Performance Test Results\n\n';
          
          // Add summary metrics
          comment += '### Summary\n';
          comment += `- Throughput: ${results.metrics.throughput.toFixed(2)} req/s\n`;
          comment += `- P95 Latency: ${results.metrics.latency_p95.toFixed(2)}ms\n`;
          comment += `- Memory Usage: ${(results.metrics.memory_usage / 1024 / 1024).toFixed(2)}MB\n\n`;
          
          // Add regression info if any
          if (Object.keys(results.regressions).length > 0) {
            comment += '### ⚠️ Regressions Detected\n';
            for (const [metric, details] of Object.entries(results.regressions)) {
              comment += `- ${metric}:\n`;
              comment += `  - Baseline: ${details.baseline.toFixed(2)}\n`;
              comment += `  - Current: ${details.current.toFixed(2)}\n`;
              comment += `  - Degradation: ${details.degradation.toFixed(1)}%\n`;
            }
          } else {
            comment += '✅ No performance regressions detected\n';
          }
          
          // Add links to detailed reports
          comment += '\n### Detailed Reports\n';
          comment += '- [HTML Report](${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID}/artifacts)\n';
          
          await github.rest.issues.createComment({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
            body: comment
          });

    - name: Handle Failed Tests
      if: failure()
      run: |
        echo "::error::Performance tests failed. Check the logs for details."
        exit 1
