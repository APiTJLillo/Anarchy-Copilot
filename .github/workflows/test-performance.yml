name: Test Performance

on:
  # Run daily at midnight UTC
  schedule:
    - cron: '0 0 * * *'
  
  # Run after any model training
  workflow_run:
    workflows: ["Model Monitoring"]
    types:
      - completed
  
  # Manual trigger
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.10'
  PERF_HISTORY_DIR: benchmark_results/performance_history

jobs:
  test-suite:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0  # Full history for trend analysis

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r tests/requirements-test.txt
        pip install pytest-asyncio pytest-cov psutil aiohttp plotly

    - name: Load test history
      uses: actions/cache@v3
      with:
        path: ${{ env.PERF_HISTORY_DIR }}
        key: perf-history-${{ github.sha }}
        restore-keys: |
          perf-history-

    - name: Create performance directories
      run: |
        mkdir -p ${{ env.PERF_HISTORY_DIR }}/unit
        mkdir -p ${{ env.PERF_HISTORY_DIR }}/stress
        mkdir -p ${{ env.PERF_HISTORY_DIR }}/load

    - name: Run unit tests
      run: |
        python -m pytest tests/test_ml_pipeline.py \
          --junitxml=test-results/unit.xml \
          --cov=scripts \
          --cov-report=xml:coverage/unit.xml \
          -v

    - name: Run stress tests
      run: |
        python -m pytest tests/test_ml_pipeline_stress.py \
          --junitxml=test-results/stress.xml \
          -v

    - name: Run load tests
      run: |
        python -m pytest tests/test_ml_pipeline_load.py \
          --junitxml=test-results/load.xml \
          -v

    - name: Generate performance report
      run: |
        python - <<EOF
        import json
        import time
        from pathlib import Path
        from datetime import datetime
        
        def save_metrics(test_type, metrics):
            history_file = Path("${{ env.PERF_HISTORY_DIR }}") / test_type / f"{time.time()}.json"
            history_file.write_text(json.dumps({
                "timestamp": datetime.now().isoformat(),
                "commit": "${{ github.sha }}",
                "metrics": metrics
            }, indent=2))
        
        # Parse test results
        from xml.etree import ElementTree
        
        def parse_junit(path):
            tree = ElementTree.parse(path)
            return {
                "tests": int(tree.getroot().attrib['tests']),
                "failures": int(tree.getroot().attrib.get('failures', 0)),
                "errors": int(tree.getroot().attrib.get('errors', 0)),
                "time": float(tree.getroot().attrib['time'])
            }
        
        # Parse coverage
        def parse_coverage(path):
            tree = ElementTree.parse(path)
            return {
                "line_rate": float(tree.getroot().attrib['line-rate']),
                "branch_rate": float(tree.getroot().attrib.get('branch-rate', 0)),
                "complexity": float(tree.getroot().attrib.get('complexity', 0))
            }
        
        # Collect metrics
        metrics = {
            "unit": {
                **parse_junit("test-results/unit.xml"),
                **parse_coverage("coverage/unit.xml")
            },
            "stress": parse_junit("test-results/stress.xml"),
            "load": parse_junit("test-results/load.xml")
        }
        
        # Save metrics by type
        for test_type, test_metrics in metrics.items():
            save_metrics(test_type, test_metrics)
        
        # Generate summary report
        summary = {
            "commit": "${{ github.sha }}",
            "timestamp": datetime.now().isoformat(),
            "pass_rate": {
                test_type: 100 * (1 - (m['failures'] + m['errors']) / m['tests'])
                for test_type, m in metrics.items()
            },
            "execution_time": {
                test_type: m['time']
                for test_type, m in metrics.items()
            },
            "coverage": metrics['unit'].get('line_rate', 0) * 100
        }
        
        Path("test-results/summary.json").write_text(
            json.dumps(summary, indent=2)
        )
        EOF

    - name: Analyze performance trends
      run: |
        python - <<EOF
        import json
        import plotly.graph_objects as go
        from plotly.subplots import make_subplots
        import pandas as pd
        from pathlib import Path
        
        def load_history(test_type):
            history = []
            history_dir = Path("${{ env.PERF_HISTORY_DIR }}") / test_type
            for file in sorted(history_dir.glob("*.json")):
                data = json.loads(file.read_text())
                history.append({
                    "timestamp": pd.Timestamp(data["timestamp"]),
                    **data["metrics"]
                })
            return pd.DataFrame(history)
        
        # Create visualization
        fig = make_subplots(
            rows=3, cols=2,
            subplot_titles=(
                "Unit Test Duration",
                "Unit Test Coverage",
                "Stress Test Duration",
                "Stress Test Pass Rate",
                "Load Test Duration",
                "Load Test Pass Rate"
            )
        )
        
        # Plot unit test metrics
        unit_df = load_history("unit")
        fig.add_trace(
            go.Scatter(x=unit_df["timestamp"], y=unit_df["time"],
                      name="Unit Duration"),
            row=1, col=1
        )
        fig.add_trace(
            go.Scatter(x=unit_df["timestamp"], y=unit_df["line_rate"] * 100,
                      name="Coverage"),
            row=1, col=2
        )
        
        # Plot stress test metrics
        stress_df = load_history("stress")
        fig.add_trace(
            go.Scatter(x=stress_df["timestamp"], y=stress_df["time"],
                      name="Stress Duration"),
            row=2, col=1
        )
        pass_rate = 100 * (1 - (stress_df["failures"] + stress_df["errors"]) / stress_df["tests"])
        fig.add_trace(
            go.Scatter(x=stress_df["timestamp"], y=pass_rate,
                      name="Stress Pass Rate"),
            row=2, col=2
        )
        
        # Plot load test metrics
        load_df = load_history("load")
        fig.add_trace(
            go.Scatter(x=load_df["timestamp"], y=load_df["time"],
                      name="Load Duration"),
            row=3, col=1
        )
        pass_rate = 100 * (1 - (load_df["failures"] + load_df["errors"]) / load_df["tests"])
        fig.add_trace(
            go.Scatter(x=load_df["timestamp"], y=pass_rate,
                      name="Load Pass Rate"),
            row=3, col=2
        )
        
        fig.update_layout(
            height=1200,
            title_text="Test Performance Trends",
            showlegend=True
        )
        
        # Save visualization
        fig.write_html("test-results/trends.html")
        EOF

    - name: Check performance budget
      run: |
        python - <<EOF
        import json
        from pathlib import Path
        
        # Load performance budget
        budget = json.loads(Path("performance-budget.json").read_text())
        summary = json.loads(Path("test-results/summary.json").read_text())
        
        # Check against budget
        violations = []
        
        def check_metric(actual, limit, metric):
            if actual > limit:
                violations.append(
                    f"{metric}: {actual:.1f} exceeds budget of {limit:.1f}"
                )
        
        check_metric(
            summary["execution_time"]["unit"],
            budget["max_unit_test_time"],
            "Unit test duration"
        )
        
        check_metric(
            summary["execution_time"]["stress"],
            budget["max_stress_test_time"],
            "Stress test duration"
        )
        
        check_metric(
            summary["execution_time"]["load"],
            budget["max_load_test_time"],
            "Load test duration"
        )
        
        # Coverage requirement
        if summary["coverage"] < budget["min_coverage"]:
            violations.append(
                f"Coverage {summary['coverage']:.1f}% below minimum {budget['min_coverage']}%"
            )
        
        # Pass rate requirements
        for test_type, rate in summary["pass_rate"].items():
            if rate < budget["min_pass_rate"]:
                violations.append(
                    f"{test_type} pass rate {rate:.1f}% below minimum {budget['min_pass_rate']}%"
                )
        
        if violations:
            print("Performance budget violations:")
            for v in violations:
                print(f"- {v}")
            exit(1)
        
        print("All performance metrics within budget!")
        EOF

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-results
        path: |
          test-results/
          coverage/

    - name: Update performance history
      if: success()
      uses: actions/cache/save@v3
      with:
        path: ${{ env.PERF_HISTORY_DIR }}
        key: perf-history-${{ github.sha }}

    - name: Comment on related pull request
      if: github.event_name == 'workflow_run'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          const summary = JSON.parse(
            fs.readFileSync('test-results/summary.json', 'utf8')
          );
          
          const body = `# Test Performance Results
          
          ## Summary
          - Unit Test Pass Rate: ${summary.pass_rate.unit.toFixed(1)}%
          - Stress Test Pass Rate: ${summary.pass_rate.stress.toFixed(1)}%
          - Load Test Pass Rate: ${summary.pass_rate.load.toFixed(1)}%
          - Coverage: ${summary.coverage.toFixed(1)}%
          
          ## Execution Times
          - Unit Tests: ${summary.execution_time.unit.toFixed(2)}s
          - Stress Tests: ${summary.execution_time.stress.toFixed(2)}s
          - Load Tests: ${summary.execution_time.load.toFixed(2)}s
          
          [View Detailed Report](${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID})`;
          
          const pulls = await github.rest.pulls.list({
            owner: context.repo.owner,
            repo: context.repo.repo,
            state: 'open'
          });
          
          for (const pull of pulls.data) {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: pull.number,
              body: body
            });
          }

    - name: Report status
      if: always()
      run: |
        if [ -f test-results/summary.json ]; then
          echo "Test suite completed with results:"
          cat test-results/summary.json
        else
          echo "Test suite failed to generate summary"
          exit 1
        fi
