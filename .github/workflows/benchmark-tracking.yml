name: Benchmark Tracking

on:
  # Run daily at midnight UTC
  schedule:
    - cron: '0 0 * * *'
  # Run on pull requests
  pull_request:
    paths:
      - 'proxy/**'
      - 'scripts/**'
      - 'tests/**'
      - '.github/workflows/benchmark-tracking.yml'
  # Allow manual triggering
  workflow_dispatch:

jobs:
  benchmark:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10"]
      # Don't cancel other jobs if one fails
      fail-fast: false

    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0  # Full history for trend analysis

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-typing.txt
        pip install plotly numpy psutil pytest-benchmark

    - name: Load previous benchmarks
      uses: actions/cache@v3
      with:
        path: benchmark_results
        key: benchmarks-${{ github.ref_name }}-${{ github.sha }}
        restore-keys: |
          benchmarks-${{ github.ref_name }}-
          benchmarks-main-
          benchmarks-

    - name: Run benchmarks
      run: |
        mkdir -p benchmark_results
        export PYTHONPATH=$PWD
        pytest tests/benchmark_type_suggestions.py --benchmark-only --benchmark-json benchmark_results/type_suggestion_benchmarks_${{ github.sha }}.json

    - name: Generate reports
      run: |
        python scripts/visualize_benchmark_trends.py
        python scripts/visualize_benchmarks.py

    - name: Check for performance regression
      id: check-regression
      run: |
        python - <<EOF
        import json
        from pathlib import Path
        import sys
        
        def load_latest_results():
            results_dir = Path("benchmark_results")
            files = sorted(results_dir.glob("type_suggestion_benchmarks_*.json"))
            if len(files) < 2:
                return None, None
            
            current = json.loads(files[-1].read_text())
            previous = json.loads(files[-2].read_text())
            return current, previous
        
        def check_regression(current, previous):
            if not (current and previous):
                print("::warning::Not enough data for regression analysis")
                return True
            
            # Check processing speed
            current_speed = current.get('medium', {}).get('data', {}).get('stats', {}).get('suggestions_per_second', 0)
            previous_speed = previous.get('medium', {}).get('data', {}).get('stats', {}).get('suggestions_per_second', 1)
            
            speed_change = (current_speed / previous_speed - 1) * 100
            
            # Check memory usage
            current_mem = current.get('memory_usage', {}).get('data', {}).get('max_memory_mb', 0)
            previous_mem = previous.get('memory_usage', {}).get('data', {}).get('max_memory_mb', 1)
            
            mem_change = (current_mem / previous_mem - 1) * 100
            
            # Define regression thresholds
            SPEED_THRESHOLD = -10  # 10% slowdown
            MEMORY_THRESHOLD = 20  # 20% memory increase
            
            has_regression = False
            
            if speed_change < SPEED_THRESHOLD:
                print(f"::error::Performance regression detected: {speed_change:.1f}% slower")
                has_regression = True
            
            if mem_change > MEMORY_THRESHOLD:
                print(f"::error::Memory regression detected: {mem_change:.1f}% increase")
                has_regression = True
            
            # Export performance change metrics
            with open("perf_changes.txt", "w") as f:
                print(f"speed_change={speed_change:.1f}", file=f)
                print(f"mem_change={mem_change:.1f}", file=f)
            
            return not has_regression
        
        current, previous = load_latest_results()
        success = check_regression(current, previous)
        sys.exit(0 if success else 1)
        EOF

    - name: Upload benchmark results
      if: success() || failure()
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: |
          benchmark_results/
          perf_changes.txt
        retention-days: 90

    - name: Save cache
      if: success() || failure()
      uses: actions/cache/save@v3
      with:
        path: benchmark_results
        key: benchmarks-${{ github.ref_name }}-${{ github.sha }}

    - name: Comment on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          // Read performance changes
          const changes = fs.readFileSync('perf_changes.txt', 'utf8')
            .split('\n')
            .filter(line => line)
            .reduce((acc, line) => {
              const [key, value] = line.split('=');
              acc[key] = parseFloat(value);
              return acc;
            }, {});
          
          const formatChange = (value) => {
            const icon = value >= 0 ? 'ðŸŸ¢' : 'ðŸ”´';
            return `${icon} ${value >= 0 ? '+' : ''}${value.toFixed(1)}%`;
          };
          
          const comment = `## Benchmark Results
          
          ### Performance Changes
          - Processing Speed: ${formatChange(changes.speed_change)}
          - Memory Usage: ${formatChange(-changes.mem_change)}
          
          [View detailed report](${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID})`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

    - name: Report Status
      if: success() || failure()
      run: |
        if [ -f perf_changes.txt ]; then
          while IFS='=' read -r key value; do
            if [[ $key == "speed_change" && $(echo "$value < -10" | bc -l) == 1 ]]; then
              echo "::warning::Significant performance regression detected!"
            fi
            if [[ $key == "mem_change" && $(echo "$value > 20" | bc -l) == 1 ]]; then
              echo "::warning::Significant memory usage increase detected!"
            fi
          done < perf_changes.txt
        fi

    - name: Check Performance Budget
      if: success() || failure()
      run: |
        python scripts/analyze_types.py --check-coverage --report
        if [ $? -ne 0 ]; then
          echo "::error::Failed to meet performance budget requirements"
          exit 1
        fi
