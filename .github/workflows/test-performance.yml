name: Test Performance

on:
  # Run daily at midnight UTC
  schedule:
    - cron: '0 0 * * *'
  
  # Run after any model training
  workflow_run:
    workflows: ["Model Monitoring"]
    types:
      - completed
  
  # Manual trigger
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.10'
  PERF_HISTORY_DIR: benchmark_results/performance_history

jobs:
  test-suite:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0  # Full history for trend analysis

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r tests/requirements-test.txt
        pip install -r tests/requirements-ml.txt
        pip install pytest-asyncio pytest-cov psutil aiohttp plotly

    - name: Load test history
      uses: actions/cache@v3
      with:
        path: ${{ env.PERF_HISTORY_DIR }}
        key: perf-history-${{ github.sha }}
        restore-keys: |
          perf-history-

    - name: Create performance directories
      run: |
        mkdir -p ${{ env.PERF_HISTORY_DIR }}/unit
        mkdir -p ${{ env.PERF_HISTORY_DIR }}/stress
        mkdir -p ${{ env.PERF_HISTORY_DIR }}/load

    - name: Run unit tests
      run: |
        python -m pytest tests/test_ml_pipeline.py \
          --junitxml=test-results/unit.xml \
          --cov=scripts \
          --cov-report=xml:coverage/unit.xml \
          -v || true

    - name: Check if stress test file exists
      run: |
        if [ -f "tests/test_ml_pipeline_stress.py" ]; then
          python -m pytest tests/test_ml_pipeline_stress.py \
            --junitxml=test-results/stress.xml \
            -v || true
        else
          echo "Stress test file not found, skipping..."
          touch test-results/stress.xml
          echo '<?xml version="1.0" encoding="UTF-8"?><testsuites><testsuite tests="0" failures="0" errors="0" time="0"/></testsuites>' > test-results/stress.xml
        fi

    - name: Check if load test file exists
      run: |
        if [ -f "tests/test_ml_pipeline_load.py" ]; then
          python -m pytest tests/test_ml_pipeline_load.py \
            --junitxml=test-results/load.xml \
            -v || true
        else
          echo "Load test file not found, skipping..."
          touch test-results/load.xml
          echo '<?xml version="1.0" encoding="UTF-8"?><testsuites><testsuite tests="0" failures="0" errors="0" time="0"/></testsuites>' > test-results/load.xml
        fi

    - name: Generate performance report
      run: |
        python - <<EOF
        import json
        import time
        from pathlib import Path
        from datetime import datetime
        
        def save_metrics(test_type, metrics):
            history_file = Path("${{ env.PERF_HISTORY_DIR }}") / test_type / f"{time.time()}.json"
            history_file.write_text(json.dumps({
                "timestamp": datetime.now().isoformat(),
                "commit": "${{ github.sha }}",
                "metrics": metrics
            }, indent=2))
        
        # Parse test results
        from xml.etree import ElementTree
        
        def parse_junit(path):
            try:
                tree = ElementTree.parse(path)
                return {
                    "tests": int(tree.getroot().attrib['tests']),
                    "failures": int(tree.getroot().attrib.get('failures', 0)),
                    "errors": int(tree.getroot().attrib.get('errors', 0)),
                    "time": float(tree.getroot().attrib['time'])
                }
            except Exception as e:
                print(f"Error parsing {path}: {e}")
                return {"tests": 0, "failures": 0, "errors": 0, "time": 0}
        
        # Parse coverage
        def parse_coverage(path):
            try:
                tree = ElementTree.parse(path)
                return {
                    "line_rate": float(tree.getroot().attrib['line-rate']),
                    "branch_rate": float(tree.getroot().attrib.get('branch-rate', 0)),
                    "complexity": float(tree.getroot().attrib.get('complexity', 0))
                }
            except Exception as e:
                print(f"Error parsing coverage {path}: {e}")
                return {"line_rate": 0, "branch_rate": 0, "complexity": 0}
        
        # Collect metrics
        metrics = {
            "unit": {
                **parse_junit("test-results/unit.xml"),
                **parse_coverage("coverage/unit.xml")
            },
            "stress": parse_junit("test-results/stress.xml"),
            "load": parse_junit("test-results/load.xml")
        }
        
        # Save metrics by type
        for test_type, test_metrics in metrics.items():
            save_metrics(test_type, test_metrics)
        
        # Generate summary report
        summary = {
            "commit": "${{ github.sha }}",
            "timestamp": datetime.now().isoformat(),
            "pass_rate": {
                test_type: 100 * (1 - (m['failures'] + m['errors']) / max(m['tests'], 1))
                for test_type, m in metrics.items()
            },
            "execution_time": {
                test_type: m['time']
                for test_type, m in metrics.items()
            },
            "coverage": metrics['unit'].get('line_rate', 0) * 100
        }
        
        Path("test-results/summary.json").write_text(
            json.dumps(summary, indent=2)
        )
        EOF

    - name: Analyze performance trends
      run: |
        python - <<EOF
        import json
        import plotly.graph_objects as go
        from plotly.subplots import make_subplots
        import pandas as pd
        from pathlib import Path
        
        def load_history(test_type):
            history = []
            history_dir = Path("${{ env.PERF_HISTORY_DIR }}") / test_type
            for file in sorted(history_dir.glob("*.json")):
                try:
                    data = json.loads(file.read_text())
                    history.append({
                        "timestamp": pd.Timestamp(data["timestamp"]),
                        **data["metrics"]
                    })
                except Exception as e:
                    print(f"Error loading history file {file}: {e}")
            return pd.DataFrame(history)
        
        # Create visualization
        fig = make_subplots(
            rows=3, cols=2,
            subplot_titles=(
                "Unit Test Duration",
                "Unit Test Coverage",
                "Stress Test Duration",
                "Stress Test Pass Rate",
                "Load Test Duration",
                "Load Test Pass Rate"
            )
        )
        
        # Plot unit test metrics
        unit_df = load_history("unit")
        if not unit_df.empty:
            fig.add_trace(
                go.Scatter(x=unit_df["timestamp"], y=unit_df["time"],
                          name="Unit Duration"),
                row=1, col=1
            )
            fig.add_trace(
                go.Scatter(x=unit_df["timestamp"], y=unit_df["line_rate"] * 100,
                          name="Coverage"),
                row=1, col=2
            )
        
        # Plot stress test metrics
        stress_df = load_history("stress")
        if not stress_df.empty:
            fig.add_trace(
                go.Scatter(x=stress_df["timestamp"], y=stress_df["time"],
                          name="Stress Duration"),
                row=2, col=1
            )
            pass_rate = 100 * (1 - (stress_df["failures"] + stress_df["errors"]) / max(stress_df["tests"], 1))
            fig.add_trace(
                go.Scatter(x=stress_df["timestamp"], y=pass_rate,
                          name="Stress Pass Rate"),
                row=2, col=2
            )
        
        # Plot load test metrics
        load_df = load_history("load")
        if not load_df.empty:
            fig.add_trace(
                go.Scatter(x=load_df["timestamp"], y=load_df["time"],
                          name="Load Duration"),
                row=3, col=1
            )
            pass_rate = 100 * (1 - (load_df["failures"] + load_df["errors"]) / max(load_df["tests"], 1))
            fig.add_trace(
                go.Scatter(x=load_df["timestamp"], y=pass_rate,
                          name="Load Pass Rate"),
                row=3, col=2
            )
        
        fig.update_layout(
            height=1200,
            title_text="Test Performance Trends",
            showlegend=True
        )
        
        # Save visualization
        fig.write_html("test-results/trends.html")
        EOF

    - name: Check performance budget
      run: |
        python - <<EOF
        import json
        from pathlib import Path
        
        # Load performance budget
        try:
            budget = json.loads(Path("performance-budget.json").read_text())
            summary = json.loads(Path("test-results/summary.json").read_text())
            
            # Check against budget
            violations = []
            
            def check_metric(actual, limit, metric):
                if actual > limit:
                    violations.append(
                        f"{metric}: {actual:.1f} exceeds budget of {limit:.1f}"
                    )
            
            # Check pass rates
            for test_type, pass_rate in summary["pass_rate"].items():
                check_metric(pass_rate, budget.get(f"{test_type}_pass_rate", 95), f"{test_type} pass rate")
            
            # Check execution times
            for test_type, duration in summary["execution_time"].items():
                check_metric(duration, budget.get(f"{test_type}_duration", 300), f"{test_type} duration")
            
            # Check coverage
            check_metric(summary["coverage"], budget.get("coverage", 80), "coverage")
            
            if violations:
                print("Performance budget violations:")
                for violation in violations:
                    print(f"- {violation}")
                exit(1)
            else:
                print("All performance metrics within budget")
        except Exception as e:
            print(f"Error checking performance budget: {e}")
            exit(1)
        EOF

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-results
        path: |
          test-results/
          coverage/

    - name: Update performance history
      if: success()
      uses: actions/cache/save@v3
      with:
        path: ${{ env.PERF_HISTORY_DIR }}
        key: perf-history-${{ github.sha }}

    - name: Comment on related pull request
      if: github.event_name == 'workflow_run'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          const summary = JSON.parse(
            fs.readFileSync('test-results/summary.json', 'utf8')
          );
          
          const body = `# Test Performance Results
          
          ## Summary
          - Unit Test Pass Rate: ${summary.pass_rate.unit.toFixed(1)}%
          - Stress Test Pass Rate: ${summary.pass_rate.stress.toFixed(1)}%
          - Load Test Pass Rate: ${summary.pass_rate.load.toFixed(1)}%
          - Coverage: ${summary.coverage.toFixed(1)}%
          
          ## Execution Times
          - Unit Tests: ${summary.execution_time.unit.toFixed(2)}s
          - Stress Tests: ${summary.execution_time.stress.toFixed(2)}s
          - Load Tests: ${summary.execution_time.load.toFixed(2)}s
          
          [View Detailed Report](${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID})`;
          
          const pulls = await github.rest.pulls.list({
            owner: context.repo.owner,
            repo: context.repo.repo,
            state: 'open'
          });
          
          for (const pull of pulls.data) {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: pull.number,
              body: body
            });
          }

    - name: Report status
      if: always()
      run: |
        if [ -f test-results/summary.json ]; then
          echo "Test suite completed with results:"
          cat test-results/summary.json
        else
          echo "Test suite failed to generate summary"
          exit 1
        fi
